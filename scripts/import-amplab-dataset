#! /usr/bin/env bash

set -e

HADOOP="$( ls -1 /opt/hadoop/*/bin/hadoop | sort | tail -n 1 )"

read -s -p "AWS key id: " key_id ; echo
read -s -p "AWS secret access key: " secret_key ; echo

S3_PREFIX="s3n://$key_id:$secret_key@big-data-benchmark/pavlo"
HDFS_PREFIX="hdfs:///amplab"

import_tiny=0
import_1node=0
import_5nodes=0

for arg in "$@" ; do
    if [ "$arg" '=' 'tiny' ] ; then
        import_tiny=1
    fi
    if [ "$arg" '=' '1node' ] ; then
        import_1node=1
    fi
    if [ "$arg" '=' '5nodes' ] ; then
        import_5nodes=1
    fi
done

for size in tiny 1node 5nodes ; do
    if [ \( "$size" '!=' 'tiny' -o "$import_tiny" '=' '0' \) -a \
         \( "$size" '!=' '1node' -o "$import_1node" '=' '0' \) -a \
         \( "$size" '!=' '5nodes' -o "$import_5nodes" '=' '0' \) ]
    then
        continue
    fi

    for format in sequence text ; do

        hdfs_data_path="$HDFS_PREFIX/$format/$size/uservisits"
        s3_data_path="$S3_PREFIX/$format/$size/uservisits"

        if ( "$HADOOP" fs -ls "$hdfs_data_path" 2> /dev/null )
        then
            continue
        fi

        "$HADOOP" distcp "$s3_data_path" "$hdfs_data_path"
    done
done

